{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e07c515",
   "metadata": {},
   "source": [
    "# Assignment 1: Linguistic Essentials and Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a993e",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Required installations:\n",
    "* Python 3\n",
    "* Pip\n",
    "* nltk library\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c5bcc260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import brown, wordnet\n",
    "from nltk.util import bigrams\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7c385",
   "metadata": {},
   "source": [
    "## Task 1.1- Finding collocations\n",
    "* Create a tool to find collocations using (1) frequency plus part-of-speech tagging (search for adjectives and nouns) and (2) hypothesis testing (see slides for Lecture 2). Use the Brown corpus (already in NLTK).\n",
    "* Consider sequences of 2 words (bigrams).\n",
    "* Generate files containing the collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1e5f2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collocation_Finder:\n",
    "    \n",
    "    def __init__(self, frequency=5, corpus=brown):\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        self.corpus = corpus.words()\n",
    "        self.corpus_length = len(self.corpus)\n",
    "        \n",
    "        self.word_counts = {}\n",
    "        for word in self.corpus:\n",
    "            if word.lower() in self.word_counts:\n",
    "                self.word_counts[word.lower()] += 1\n",
    "            else:\n",
    "                self.word_counts[word.lower()] = 1\n",
    "\n",
    "        self.finder = BigramCollocationFinder.from_words(self.corpus)\n",
    "        self.finder.apply_freq_filter(frequency)\n",
    "        common_bigrams = finder.nbest(bigram_measures.pmi, 10000)\n",
    "\n",
    "        tagged_bigrams = [nltk.pos_tag(bigram) for bigram in common_bigrams]\n",
    "        \n",
    "        self.accepted_pos = [\"NN\", \"JJ\"]\n",
    "        self.frequency_pos_bigrams = [bigram for bigram in tagged_bigrams if self._check_pos_in_ngram(bigram, accepted_pos)]\n",
    "        \n",
    "    def _check_pos_in_ngram(self, ngram, accepted):\n",
    "        \"\"\"(string, ..., string), list(string) -> bool\"\"\"\n",
    "        for _, pos in ngram:\n",
    "            found = False\n",
    "            for word_class in accepted:\n",
    "                if word_class in pos:\n",
    "                    found = True\n",
    "            if not found: # if a word is wrong we immediately return\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    def get_frequency_pos_tagged(self):\n",
    "        return self.frequency_pos_bigrams\n",
    "\n",
    "    def _t_test(self, bigram, confidence):\n",
    "        # look bigram up in frequency dictionary\n",
    "        sample_mean = finder.ngram_fd[bigram] / (corpus_length - 1)\n",
    "\n",
    "        word1, word2 = bigram\n",
    "\n",
    "        # find the mean of distribution\n",
    "        mean_of_distribution = (word_counts[word1] / corpus_length) * (word_counts[word2] / corpus_length)\n",
    "\n",
    "        t = (sample_mean - mean_of_distribution) / (math.sqrt(sample_mean * (1 - sample_mean)) / corpus_length)\n",
    "\n",
    "        return t > confidence \n",
    "\n",
    "    def get_hypothesis_tested(self, confidence=2.576):\n",
    "        t_tested_bigrams = []\n",
    "        for bigram in self.frequency_pos_bigrams:\n",
    "            if self._t_test((bigram[0][0], bigram[1][0]), confidence):\n",
    "                t_tested_bigrams.append(bigram)\n",
    "        return t_tested_bigrams\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a016b1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 335/335 [00:00<00:00, 1248970.52it/s]\n",
      "100%|████████████████████████████████████| 335/335 [00:00<00:00, 1643382.27it/s]\n"
     ]
    }
   ],
   "source": [
    "cf = Collocation_Finder(frequency=10)\n",
    "\n",
    "#write to file\n",
    "f = open('1.1.frequency_pos_tagged.txt', 'w')\n",
    "for bigram in tqdm(cf.get_frequency_pos_tagged()):\n",
    "    f.write(bigram[0][0].lower() + \" \" + bigram[1][0].lower())\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "g = open('1.1.hypothesis_testing.txt', 'w')\n",
    "for bigram in tqdm(cf.get_hypothesis_tested()):\n",
    "    g.write(bigram[0][0].lower() + \" \" + bigram[1][0].lower())\n",
    "    g.write('\\n')\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85725a7b",
   "metadata": {},
   "source": [
    "## Task 1.2 - Correction Tool\n",
    "* Create a simple tool that corrects non-natural expressions. In detail, your tool should receive as input two or three words. If there is a collocation in your files such that the i-th word is a synonym of the i-th word given as input then the algorithm will output the first such collocation in your files (consider that two words that are the same are synonyms). For example, if it receives “powerful tea” and “strong tea” is in your list then the algorithm should print “strong tea”.\n",
    "\n",
    "* Suggestion: Use WordNet to detect synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "13c48787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correction_Tool:\n",
    "    \n",
    "    # defaults to use the collocations from the hypothesis testing in 1.1\n",
    "    def __init__(self, path=\"1.1.hypothesis_testing.txt\"):\n",
    "        with open(path, \"r\") as file:\n",
    "            collocs_raw = file.read()\n",
    "        # take the raw string and split it up into collocations\n",
    "        # check for last line in file which is just \\n.\n",
    "        self.collocations = [bigram.split()for bigram in collocs_raw.split('\\n') if bigram != '']\n",
    "        self.collocations_first_word_set = set(bigram[0] for bigram in self.collocations)\n",
    "\n",
    "    def find_synonyms(self, word):\n",
    "        \"\"\"string -> list[string]\"\"\"\n",
    "        synonym_names = []\n",
    "        for synset in wordnet.synsets(word):\n",
    "            synonym_names.extend(synset.lemma_names())\n",
    "        return synonym_names\n",
    "\n",
    "    def correct_bigram(self, bigram):\n",
    "        \"\"\"(string, string) -> (string, string)\"\"\"\n",
    "        first_word, second_word = bigram\n",
    "        for first_word_synonym in self.find_synonyms(first_word):\n",
    "            if first_word_synonym in self.collocations_first_word_set:\n",
    "                # create set of second words in known collocation list\n",
    "                # that has the synonym of the first word we are looking for as\n",
    "                # as its pairing\n",
    "                collocations_second_word_set = set(bigram[1] for bigram in self.collocations if bigram[0] == first_word_synonym)\n",
    "                for collocation_second_word in collocations_second_word_set:\n",
    "                    # the collocation we are correcting is already in the known collocation list\n",
    "                    if first_word + \" \" + second_word == first_word_synonym +  \" \" + collocation_second_word:\n",
    "                        print(first_word + \" \" + second_word + \" is already a known collocation\")\n",
    "                        return (first_word, second_word)\n",
    "                        \n",
    "                    for second_word_synonym in self.find_synonyms(second_word):\n",
    "                        if second_word_synonym == collocation_second_word:\n",
    "                            print(\"Corrected:\", first_word + \" \" + second_word, \"to\", first_word_synonym + \" \" + collocation_second_word)\n",
    "                            return (first_word_synonym, collocation_second_word)\n",
    "        return (first_word, second_word)\n",
    "    \n",
    "    def correct_sentence(self, sentence):\n",
    "        \"\"\"string -> string\"\"\"\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        for i in range(len(tokenized_sentence) - 1):\n",
    "            bigram = tokenized_sentence[i:i+2]\n",
    "            corrected_first, _ = self.correct_bigram(bigram)\n",
    "            if corrected_first != tokenized_sentence[i]:\n",
    "                tokenized_sentence[i] = corrected_first\n",
    "        \n",
    "        print(\" \".join(tokenized_sentence))  \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "38ef072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected: large man to big man\n",
      "big man thing\n",
      "Corrected: following year to next year\n",
      "expecting next year\n",
      "young people is already a known collocation\n",
      "wrong young people\n",
      "Corrected: earth war to world war\n",
      "Corrected: inner revenue to internal revenue\n",
      "world war in history with internal revenue\n"
     ]
    }
   ],
   "source": [
    "c = Correction_Tool()\n",
    "c.correct_sentence(\"large man thing\")\n",
    "c.correct_sentence(\"expecting following year\")\n",
    "c.correct_sentence(\"wrong young people\")\n",
    "c.correct_sentence(\"earth war in history with inner revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e222f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
