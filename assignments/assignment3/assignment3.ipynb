{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc0bc9",
   "metadata": {},
   "source": [
    "## 1.2 Language Models\n",
    "* Adapt your implementation of logistic regression from Assignment 2 to the implementation of a feedforward neural network (or create a new implementation from scratch). If you use an online implementation as a reference state that in your report and the modifications you have made.\n",
    "* Your neural network should predict upcoming words from prior word context (see Section 7.5 of the reference book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "899a55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from random import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35bfae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def backward_ReLU(x):\n",
    "    # found on stack overflow\n",
    "    return (x > 0) * 1\n",
    "\n",
    "def forward_softmax(z):\n",
    "    return np.array([(exp(zi)/sum([exp(zj) for zj in z])) for zi in z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1558f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Layer:\n",
    "    \"\"\"Fully Connected Layer without bias.\n",
    "       input x weights -> output\"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_output, seed=1):\n",
    "        ran = np.random\n",
    "        ran.seed(seed)\n",
    "        self.weights = ran.random_sample((n_output, n_input))\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        return np.matmul(self.weights, x.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51bb4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fully_Connected_Layer_Bias:\n",
    "    \"\"\"Fully Connected Layer with bias.\n",
    "       input x weights + bias -> output\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_output, seed=1):\n",
    "        ran = np.random\n",
    "        ran.seed(seed)\n",
    "        self.weights= ran.random_sample((n_output, n_input))\n",
    "        self.bias = ran.random_sample(n_output)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        print(x.shape, self.weights.shape)\n",
    "        x_w = np.matmul(self.weights, x.T)\n",
    "        x_w_b = x_w + self.bias\n",
    "        return x_w_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "848ef4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer Neural Network\n",
    "class Neural_Network:\n",
    "\n",
    "    # initialize all necessary informatio\n",
    "    def __init__(self, n_vocab, n_embeddings, learning_rate=0.1, n_prev_words=3):\n",
    "        \n",
    "        # information about the model\n",
    "        self.embedding_layer = Embedding_Layer(n_vocab, n_embeddings)\n",
    "        self.hidden_layer = Fully_Connected_Layer_Bias(n_embeddings * n_prev_words, n_embeddings)\n",
    "        self.output_layer = Fully_Connected_Layer_Bias(n_embeddings, n_vocab)\n",
    "        self.gradients= {}\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "        # z-values representing the results of each layer\n",
    "        # before activating them, generally means when\n",
    "        # the input has benn mutiplied by its weights\n",
    "        self.embedding_z = None\n",
    "        self.hidden_z =  None\n",
    "        self.output_z = None\n",
    "        \n",
    "        self.embedding_activation = None\n",
    "        self.hidden_activation = None\n",
    "        self.output_activation = None\n",
    "        \n",
    "        # information about the data\n",
    "        # is updated when \n",
    "        self.vocab = None\n",
    "        self.one_hot_vectors = None\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.n_prev_words = n_prev_words\n",
    "\n",
    "    # Maybe use for prediction and encoding data\n",
    "    def _create_vocab(self, x):\n",
    "        new_vocab = []\n",
    "        for word in x:\n",
    "            if word not in new_vocab:\n",
    "                new_vocab.append(x)\n",
    "        self.vocab = new_vocab\n",
    "        self.one_hot_vectors = [np.zeros(len(self.vocab)) for _ in range(len(self.vocab))]\n",
    "        for i in range(len(self.vocab)):\n",
    "            self.one_hot_vectors[i][i] = 1  \n",
    "\n",
    "    # updates layers forward\n",
    "    def _feedforward(self, x):\n",
    "        \n",
    "        self.x = x\n",
    "        \"\"\"Method from 7.5.1 in Speech and Language Processing.\n",
    "           Step by step commented for easy comparison.\"\"\"\n",
    "        e = np.array([self.embedding_layer.feedforward(x[i]) for i in range(self.n_prev_words)])\n",
    "        # create embedding layer e\n",
    "        self.embedding_z = np.concatenate(e, axis=0)\n",
    "        self.embedding_activation = self.embedding_z\n",
    "        # Multiply by W and pass through ReLU activation function\n",
    "        self.hidden_z = self.hidden_layer.feedforward(self.embedding_activation)\n",
    "        self.hidden_activation = forward_ReLU(self.hidden_z)\n",
    "        \n",
    "        # Multiply by U and apply softmax reshaping it into |Vocabulary| x 1\n",
    "        self.output_z = self.output_layer.feedforward(self.hidden_activation)\n",
    "        self.output_activation = forward_softmax(self.output_z).reshape(1, self.n_vocab)\n",
    "\n",
    "    def _backprop(self, y):\n",
    "        # step by step what to dodo\n",
    "        \n",
    "        # Find the bias of the output layer\n",
    "        # derivative of cross entropy: y_pred - y\n",
    "        self.dL_db_output = (self.output_activation - y) * self.output_activation\n",
    "        self.dL_dw_output = np.matmul(self.dL_db_output.T, self.hidden_activation.reshape(1, -1))\n",
    "\n",
    "        output_sum = np.sum(np.matmul(self.dL_db_output, self.output_layer.weights), axis=1, keepdims=True)\n",
    "        self.hidden_bReLU = backward_ReLU(self.hidden_z)\n",
    "        self.dL_db_hidden = np.matmul(output_sum.reshape(-1, 1), self.hidden_bReLU.reshape(1, -1))\n",
    "        self.dL_dw_hidden = np.matmul(self.dL_db_hidden.T, self.embedding_activation.reshape(1, -1))\n",
    "        \n",
    "        hidden_sum = np.sum(np.matmul(self.dL_db_hidden, self.hidden_layer.weights), axis=1, keepdims=True)\n",
    "        delta = np.matmul(hidden_sum.reshape(-1, 1), self.embedding_z.reshape(1, -1))\n",
    "        q = int(delta.shape[1] / self.n_prev_words)\n",
    "        self.dL_db_embedding = np.array([delta[:, i: i+q] for i in range(self.n_prev_words)]).reshape(self.n_prev_words, self.n_embeddings)\n",
    "        self.dL_dw_embedding = np.array([np.matmul(self.dL_db_embedding.reshape(-1, 1), self.x[i].reshape(1, -1)) for i in range(self.n_prev_words)])\n",
    "        \n",
    "        # finally, update the weights with the new loss gradients\n",
    "        self._update_weights()\n",
    "    \n",
    "    def _update_weights(self):\n",
    "        # update all the embedding weights\n",
    "        for i in range(self.n_prev_words):\n",
    "            self.embedding_layer.weights -= self.alpha * self.dL_dw_embedding[i]\n",
    "        # update hidden layer\n",
    "        self.hidden_layer.weights -= self.alpha * self.dL_dw_hidden\n",
    "        self.hidden_layer.bias -= self.alpha * self.dL_db_hidden\n",
    "        # update output layer\n",
    "        self.output_layer.weights -= self.alpha * self.dL_dw_output\n",
    "        self.output_layer.bias -= self.alpha * self.dL_db_output\n",
    "\n",
    "    def fit(self, x, y, n_epochs=1):\n",
    "        for x, y in zip(x, y):\n",
    "            self._feedforward(x)\n",
    "            y_one_hot = np.zeros(self.n_vocab)\n",
    "            y_one_hot[y] = 1\n",
    "            self._backprop(y_one_hot)\n",
    "            \n",
    "        return self.output_activation\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Use logistic regression\n",
    "        print(\"TODO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "770c804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,) (2, 6)\n",
      "(2,) (6, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,6) (6,6) (2,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_35950/279932127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_35950/3081432283.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, n_epochs)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0my_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_35950/3081432283.py\u001b[0m in \u001b[0;36m_backprop\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# finally, update the weights with the new loss gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_35950/3081432283.py\u001b[0m in \u001b[0;36m_update_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# update all the embedding weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_prev_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdL_dw_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# update hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdL_dw_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,6) (6,6) (2,6) "
     ]
    }
   ],
   "source": [
    "data = \"\"\"I've seen what I become, and I cannot let that happen. And for this, you join him? Your destiny can\n",
    "        change just as quickly as the love in one's heart can fade. Nothing is set in stone. But I will cause\n",
    "        so much pain. If there is to be balance, what you have seen must be forgotten. If you're not with me,\n",
    "        then you're my enemy. Only a Sith deals in absolutes. I will do what I must. You will try. It's over,\n",
    "        Anakin! I have the high ground. You underestimate my power. Don't try it!\"\"\"\n",
    "\n",
    "x = np.array([[[0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]]])\n",
    "\n",
    "y = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "model = Neural_Network(6, 2)\n",
    "result = model.fit(x, y)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6e813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
