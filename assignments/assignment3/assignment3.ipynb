{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc0bc9",
   "metadata": {},
   "source": [
    "## 1.2 Language Models\n",
    "* Adapt your implementation of logistic regression from Assignment 2 to the implementation of a feedforward neural network (or create a new implementation from scratch). If you use an online implementation as a reference state that in your report and the modifications you have made.\n",
    "* Your neural network should predict upcoming words from prior word context (see Section 7.5 of the reference book)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42285ec6",
   "metadata": {},
   "source": [
    "## Imports and installation\n",
    "\n",
    "To use this notebook properly you need to Python, as well as numpy and tqdm.\n",
    "To install numpy you can simply \"pip install numpy\", the same goes for tqdm with \"pip install tqdm\"\n",
    "\n",
    "tqdm has not been used to perform any calculations, just enclosed the for loops that are training the models to easily track progress. If you do not wish to use tqdm but still want to run the code, then simply remove the function from the batch_training() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899a55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from random import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "848ef4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    \"\"\"Neural Network with 3 layers, embedding, hidden and output\"\"\"\n",
    "    \n",
    "    class _Embedding_Layer:\n",
    "        \"\"\"Fully Connected Layer without bias.\n",
    "           input x weights -> output\"\"\"\n",
    "\n",
    "        def __init__(self, n_input, n_output, seed=1):\n",
    "            ran = np.random\n",
    "            ran.seed(seed)\n",
    "            self.weights = ran.random_sample((n_output, n_input))\n",
    "\n",
    "        def feedforward(self, x):\n",
    "            return np.matmul(self.weights, x.T)\n",
    "        \n",
    "    class _Fully_Connected_Layer:\n",
    "        \"\"\"Fully Connected Layer with bias.\n",
    "           input x weights + bias -> output\"\"\"\n",
    "\n",
    "        def __init__(self, n_input, n_output, seed=1):\n",
    "            ran = np.random\n",
    "            ran.seed(seed)\n",
    "            self.weights= ran.random_sample((n_output, n_input))\n",
    "            self.bias = ran.random_sample(n_output)\n",
    "\n",
    "        def feedforward(self, x):\n",
    "            x_w = np.matmul(self.weights, x.T)\n",
    "            x_w_b = x_w + self.bias\n",
    "            return x_w_b\n",
    "    \n",
    "    # methods used to calculate the activation layers\n",
    "    \n",
    "    def _forward_ReLU(self, x):\n",
    "        # found on stack overflow\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _backward_ReLU(self, x):\n",
    "        # found on stack overflow\n",
    "        return (x > 0)\n",
    "\n",
    "    def _forward_softmax(self, x):\n",
    "        maxx = np.max(x, axis=0, keepdims=True)\n",
    "        exp = np.exp(x - maxx)\n",
    "        summ = np.sum(exp, axis=0, keepdims=True)\n",
    "        return (exp / summ)\n",
    "\n",
    "    # initialize all necessary information\n",
    "    def __init__(self, n_vocab, n_embeddings, learning_rate=0.01, n_prev_words=3):\n",
    "        \n",
    "        # information about the model\n",
    "        self.embedding_layer = self._Embedding_Layer(n_vocab, n_embeddings)\n",
    "        self.hidden_layer = self._Fully_Connected_Layer(n_embeddings * n_prev_words, n_embeddings)\n",
    "        self.output_layer = self._Fully_Connected_Layer(n_embeddings, n_vocab)\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "        # z-values representing the results of each layer\n",
    "        # before activating them, generally means when\n",
    "        # the input has benn mutiplied by its weights\n",
    "        self.embedding_z = None\n",
    "        self.hidden_z =  None\n",
    "        self.output_z = None\n",
    "        \n",
    "        # activation of each layer\n",
    "        self.embedding_activation = None\n",
    "        self.hidden_activation = None\n",
    "        self.output_activation = None\n",
    "        \n",
    "        # information about the data\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.n_prev_words = n_prev_words\n",
    "\n",
    "    # updates layers forward\n",
    "    def _feedforward(self, x):\n",
    "        \"\"\"Method from 7.5.1 in Speech and Language Processing.\n",
    "           Step by step commented for easy comparison.\"\"\"\n",
    "        \n",
    "        # create embedding matrix\n",
    "        e = np.array([self.embedding_layer.feedforward(x[i]) for i in range(self.n_prev_words)])\n",
    "        \n",
    "        # create embedding layer e\n",
    "        self.embedding_z = np.concatenate(e, axis=0)\n",
    "        self.embedding_activation = self.embedding_z\n",
    "        # Multiply by W and pass through ReLU activation function\n",
    "        self.hidden_z = self.hidden_layer.feedforward(self.embedding_activation)\n",
    "        self.hidden_activation = self._forward_ReLU(self.hidden_z)\n",
    "        \n",
    "        # Multiply by U and apply softmax reshaping it into |Vocabulary| x 1\n",
    "        self.output_z = self.output_layer.feedforward(self.hidden_activation)\n",
    "        self.output_activation = self._forward_softmax(self.output_z).reshape(1, self.n_vocab)\n",
    "\n",
    "    def _backprop(self, x, y):\n",
    "        \n",
    "        # Find the bias of the output layer\n",
    "        # derivative of cross entropy: y_pred - y\n",
    "        self.dL_db_output = (self.output_activation - y) * self.output_activation\n",
    "        self.dL_dw_output = np.matmul(self.dL_db_output.T, self.hidden_activation.reshape(1, -1))\n",
    "\n",
    "        # use the sum of output layer bias and weights and backwards relu to find hidden layer bias\n",
    "        output_sum = np.sum(np.matmul(self.dL_db_output, self.output_layer.weights), axis=1, keepdims=True)\n",
    "        self.hidden_bReLU = self._backward_ReLU(self.hidden_z)\n",
    "        self.dL_db_hidden = np.matmul(output_sum.reshape(-1, 1), self.hidden_bReLU.reshape(1, -1))\n",
    "        self.dL_dw_hidden = np.matmul(self.dL_db_hidden.T, self.embedding_activation.reshape(1, -1))\n",
    "        \n",
    "        # use the hidden layer weights and bias to calculate the weights of the embeddings layer\n",
    "        hidden_sum = np.sum(np.matmul(self.dL_db_hidden, self.hidden_layer.weights), axis=1, keepdims=True)\n",
    "        delta = np.matmul(hidden_sum.reshape(-1, 1), self.embedding_z.reshape(1, -1))\n",
    "        q = int(delta.shape[1] / self.n_prev_words)\n",
    "        self.dL_db_embedding = np.array([delta[:, i: i+q] for i in range(self.n_prev_words)]).reshape(self.n_prev_words, self.n_embeddings)\n",
    "        self.dL_dw_embedding = np.array([np.matmul(self.dL_db_embedding[i].reshape(-1, 1), x[i].reshape(1, -1)) for i in range(self.n_prev_words)])\n",
    "        \n",
    "        # finally, update the weights with the new loss gradients\n",
    "        self._update_weights()\n",
    "    \n",
    "    def _update_weights(self):\n",
    "        # clip embeddings to avoid overflow due to normalizing not working\n",
    "        \n",
    "        # update all the embedding weights per previous word\n",
    "        for i in range(self.n_prev_words):\n",
    "            self.embedding_layer.weights -= self.alpha * np.clip(self.dL_dw_embedding[i], -1, 1)\n",
    "            \n",
    "        # update hidden layer weights and bias\n",
    "        self.hidden_layer.weights -= self.alpha * np.clip(self.dL_dw_hidden, -1, 1)\n",
    "        self.hidden_layer.bias -= self.alpha * np.clip(self.dL_db_hidden.flatten(), -1, 1)\n",
    "        \n",
    "        # update output layer weights and bias\n",
    "        self.output_layer.weights -= self.alpha * np.clip(self.dL_dw_output, -1, 1)\n",
    "        self.output_layer.bias -= self.alpha * np.clip(self.dL_db_output.flatten(), -1, 1)\n",
    "\n",
    "    def fit(self, x, y, n_epochs=1):\n",
    "        self._feedforward(x)\n",
    "        self._backprop(x, y)\n",
    "            \n",
    "        return self.output_activation\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self._feedforward(x)\n",
    "        return self.output_layer_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ba6e813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22041\n",
      "2805\n"
     ]
    }
   ],
   "source": [
    "with open(\"metamorphosis.txt\",'r',encoding='utf-8') as file:\n",
    "    raw_metamorphosis = file.read()\n",
    "    \n",
    "replacements = ['\\n', '\\t', '\\r', ';', ':', '.', ',', '\"', \"!\", '?', '-', \"'\", '  ']\n",
    "\n",
    "stripped_metamorphosis = raw_metamorphosis\n",
    "\n",
    "for rep in replacements:\n",
    "    stripped_metamorphosis = stripped_metamorphosis.replace(rep, ' ')\n",
    "\n",
    "stripped_metamorphosis = stripped_metamorphosis.lower().split()\n",
    "\n",
    "metamorphosis_vocab = list(set(stripped_metamorphosis))\n",
    "meta_vocab_dic = {}\n",
    "one_hots = {}\n",
    "for i, word in enumerate(metamorphosis_vocab):\n",
    "    one_hot_vector = np.zeros(len(metamorphosis_vocab))\n",
    "    one_hot_vector[i] = 1\n",
    "    meta_vocab_dic[word] = one_hot_vector\n",
    "\n",
    "print(len(stripped_metamorphosis))\n",
    "print(len(meta_vocab_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41528194",
   "metadata": {},
   "source": [
    "## Create datasets and vocab dictionaries\n",
    "As done in Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f21900f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75453\n",
      "7360\n"
     ]
    }
   ],
   "source": [
    "with open(\"frankenstein.txt\",'r',encoding='utf-8') as file:\n",
    "    raw_frankenstein = file.read()\n",
    "    \n",
    "replacements = ['\\n', '\\t', '\\r', ';', ':', '.', ',', '\"', \"!\", '?', '-', \"'\", '  ']\n",
    "\n",
    "stripped_frankenstein = raw_frankenstein\n",
    "\n",
    "for rep in replacements:\n",
    "    stripped_frankenstein = stripped_frankenstein.replace(rep, ' ')\n",
    "\n",
    "stripped_frankenstein = stripped_frankenstein.lower().split()\n",
    "\n",
    "frankenstein_vocab = list(set(stripped_frankenstein))\n",
    "frank_vocab_dic = {}\n",
    "one_hots = {}\n",
    "for i, word in enumerate(frankenstein_vocab):\n",
    "    one_hot_vector = np.zeros(len(frankenstein_vocab))\n",
    "    one_hot_vector[i] = 1\n",
    "    frank_vocab_dic[word] = one_hot_vector\n",
    "\n",
    "print(len(stripped_frankenstein))\n",
    "print(len(frank_vocab_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa9e5c",
   "metadata": {},
   "source": [
    "## Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a583d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training(model, dataset, vocab, batch_size=5000, n_prev_words=3):\n",
    "    \"\"\"Method that trains a model, its dataet and vocabulary in batches to\n",
    "    avoid memory overflow and give feedback during the training\"\"\"\n",
    "    n_vocab = len(vocab)\n",
    "    n_dataset = len(dataset) - n_prev_words\n",
    "    n_batches = n_dataset // batch_size\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        batch_len = batch_size - n_prev_words\n",
    "        data = np.zeros((batch_len, n_prev_words, n_vocab), dtype='float16')\n",
    "        labels = np.zeros((batch_len, n_vocab), dtype='int16')\n",
    "        for i in range(batch_size - n_prev_words):\n",
    "            x = np.zeros((n_prev_words, n_vocab))\n",
    "            for j in range(n_prev_words):\n",
    "                x[j] = vocab[dataset[(batch*batch_size):(batch*batch_size)+batch_size][i+j]]\n",
    "            data[i] = x\n",
    "            labels[i] = vocab[dataset[(batch*batch_size):(batch*batch_size)+batch_size][i+n_prev_words]]\n",
    "\n",
    "        for x, y in tqdm(zip(data, labels)):\n",
    "            result = model.fit(x, y)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8785e430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [00:33, 149.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00048461 0.00019245 0.00038576 ... 0.00031336 0.0002925  0.00039608]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [00:34, 145.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00050185 0.00024333 0.00043264 ... 0.00023531 0.00036686 0.00045951]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [00:37, 134.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00052805 0.00026207 0.00037034 ... 0.00022189 0.00039551 0.0004552 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [00:32, 155.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00054611 0.00023173 0.00043494 ... 0.00023997 0.00034586 0.0004149 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [01:57, 59.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.51868655e-05 1.01136450e-04 7.73743662e-05 ... 1.45184293e-04\n",
      "  1.36807658e-04 1.44104869e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [01:58, 58.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.71578858e-05 1.71733248e-04 1.04465026e-04 ... 3.51690689e-04\n",
      "  4.08450699e-05 1.05157456e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [02:00, 57.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.13476230e-04 1.18323857e-04 9.53388106e-05 ... 1.76735250e-04\n",
      "  1.28956548e-04 1.33306876e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [01:59, 58.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.01195393e-04 7.96782763e-05 6.91902425e-05 ... 2.32026968e-04\n",
      "  1.30209744e-04 1.39806600e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [01:58, 59.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.10953858e-04 1.17285311e-04 9.59584451e-05 ... 1.73749217e-04\n",
      "  1.29756092e-04 1.31393169e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [02:01, 57.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.28951884e-04 1.20730306e-04 8.14509153e-05 ... 1.82884433e-04\n",
      "  1.31674030e-04 1.28150496e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [01:57, 59.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00014938 0.0001984  0.00010205 ... 0.00016767 0.00011821 0.00011059]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [02:02, 56.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00011006 0.00011831 0.00010282 ... 0.00016098 0.0001375  0.00012448]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [01:52, 62.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.13912930e-04 1.21513082e-04 9.00459325e-05 ... 1.63825036e-04\n",
      "  1.22396469e-04 1.26048449e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [02:22, 48.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.09850807e-04 1.18779548e-04 9.68671124e-05 ... 1.75219342e-04\n",
      "  1.30749421e-04 1.32834846e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metamorphosis_model = Neural_Network(len(meta_vocab_dic), 64)\n",
    "batch_training(metamorphosis_model, stripped_metamorphosis, meta_vocab_dic)\n",
    "\n",
    "frankenstein_model = Neural_Network(len(frank_vocab_dic), 64)\n",
    "batch_training(frankenstein_model, stripped_frankenstein, frank_vocab_dic, batch_size=7000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330d778",
   "metadata": {},
   "source": [
    "## Store the embeddings in TSV files\n",
    "This is done so we get files that we can use in the Embedding Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a961723",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_metamorphosis_v = open('vectors_m.tsv', 'w', encoding='utf-8')\n",
    "out_metamorphosis_m = open('metadata_m.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(metamorphosis_vocab):\n",
    "    # get the index of all the dimensions to form a single vector\n",
    "    out_metamorphosis_v.write('\\t'.join([str(x) for x in metamorphosis_model.embedding_layer.weights[:, index]]) + \"\\n\")\n",
    "    out_metamorphosis_m.write(word + \"\\n\")\n",
    "out_metamorphosis_v.close()\n",
    "out_metamorphosis_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64830e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_frankenstein_v = open('vectors_f.tsv', 'w', encoding='utf-8')\n",
    "out_frankenstein_m = open('metadata_f.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(frankenstein_vocab):\n",
    "    out_frankenstein_v.write('\\t'.join([str(x) for x in frankenstein_model.embedding_layer.weights[:, index]]) + \"\\n\")\n",
    "    out_frankenstein_m.write(word + \"\\n\")\n",
    "out_frankenstein_v.close()\n",
    "out_frankenstein_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e37e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
