{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc0bc9",
   "metadata": {},
   "source": [
    "## 1.2 Language Models\n",
    "* Adapt your implementation of logistic regression from Assignment 2 to the implementation of a feedforward neural network (or create a new implementation from scratch). If you use an online implementation as a reference state that in your report and the modifications you have made.\n",
    "* Your neural network should predict upcoming words from prior word context (see Section 7.5 of the reference book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "899a55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from random import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35bfae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def backward_ReLU(x):\n",
    "    # found on stack overflow\n",
    "    return (x > 0)\n",
    "\n",
    "def forward_softmax(x):\n",
    "    max = np.max(x, axis=0, keepdims=True)\n",
    "    e_x = np.exp(x - max)\n",
    "    sum = np.sum(e_x, axis=0, keepdims=True)\n",
    "    f_x = e_x / sum\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1558f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Layer:\n",
    "    \"\"\"Fully Connected Layer without bias.\n",
    "       input x weights -> output\"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_output, seed=1):\n",
    "        ran = np.random\n",
    "        ran.seed(seed)\n",
    "        self.weights = ran.random_sample((n_output, n_input))\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        return np.matmul(self.weights, x.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bb4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fully_Connected_Layer_Bias:\n",
    "    \"\"\"Fully Connected Layer with bias.\n",
    "       input x weights + bias -> output\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_output, seed=1):\n",
    "        ran = np.random\n",
    "        ran.seed(seed)\n",
    "        self.weights= ran.random_sample((n_output, n_input))\n",
    "        self.bias = ran.random_sample(n_output)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        x_w = np.matmul(self.weights, x.T)\n",
    "        x_w_b = x_w + self.bias\n",
    "        return x_w_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "848ef4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer Neural Network\n",
    "class Neural_Network:\n",
    "\n",
    "    # initialize all necessary informatio\n",
    "    def __init__(self, n_vocab, n_embeddings, learning_rate=0.1, n_prev_words=3):\n",
    "        \n",
    "        # information about the model\n",
    "        self.embedding_layer = Embedding_Layer(n_vocab, n_embeddings)\n",
    "        self.hidden_layer = Fully_Connected_Layer_Bias(n_embeddings * n_prev_words, n_embeddings)\n",
    "        self.output_layer = Fully_Connected_Layer_Bias(n_embeddings, n_vocab)\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "        # z-values representing the results of each layer\n",
    "        # before activating them, generally means when\n",
    "        # the input has benn mutiplied by its weights\n",
    "        self.embedding_z = None\n",
    "        self.hidden_z =  None\n",
    "        self.output_z = None\n",
    "        \n",
    "        self.embedding_activation = None\n",
    "        self.hidden_activation = None\n",
    "        self.output_activation = None\n",
    "        \n",
    "        # information about the data\n",
    "        # is updated when \n",
    "        self.vocab = None\n",
    "        self.one_hot_vectors = None\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.n_prev_words = n_prev_words\n",
    "\n",
    "    # Maybe use for prediction and encoding data\n",
    "    def _create_vocab(self, x):\n",
    "        new_vocab = []\n",
    "        for word in x:\n",
    "            if word not in new_vocab:\n",
    "                new_vocab.append(x)\n",
    "        self.vocab = new_vocab\n",
    "        self.one_hot_vectors = [np.zeros(len(self.vocab)) for _ in range(len(self.vocab))]\n",
    "        for i in range(len(self.vocab)):\n",
    "            self.one_hot_vectors[i][i] = 1  \n",
    "\n",
    "    # updates layers forward\n",
    "    def _feedforward(self, x):\n",
    "        \n",
    "        self.x = x\n",
    "        \"\"\"Method from 7.5.1 in Speech and Language Processing.\n",
    "           Step by step commented for easy comparison.\"\"\"\n",
    "        e = np.array([self.embedding_layer.feedforward(x[i]) for i in range(self.n_prev_words)])\n",
    "        # create embedding layer e\n",
    "        self.embedding_z = np.concatenate(e, axis=0)\n",
    "        self.embedding_activation = self.embedding_z\n",
    "        # Multiply by W and pass through ReLU activation function\n",
    "        self.hidden_z = self.hidden_layer.feedforward(self.embedding_activation)\n",
    "        self.hidden_activation = forward_ReLU(self.hidden_z)\n",
    "        \n",
    "        # Multiply by U and apply softmax reshaping it into |Vocabulary| x 1\n",
    "        self.output_z = self.output_layer.feedforward(self.hidden_activation)\n",
    "        self.output_activation = forward_softmax(self.output_z).reshape(1, self.n_vocab)\n",
    "\n",
    "    def _backprop(self, y):\n",
    "        # step by step what to dodo\n",
    "        \n",
    "        # Find the bias of the output layer\n",
    "        # derivative of cross entropy: y_pred - y\n",
    "        self.dL_db_output = (self.output_activation - y) * self.output_activation\n",
    "        self.dL_dw_output = np.matmul(self.dL_db_output.T, self.hidden_activation.reshape(1, -1))\n",
    "\n",
    "        output_sum = np.sum(np.matmul(self.dL_db_output, self.output_layer.weights), axis=1, keepdims=True)\n",
    "        self.hidden_bReLU = backward_ReLU(self.hidden_z)\n",
    "        self.dL_db_hidden = np.matmul(output_sum.reshape(-1, 1), self.hidden_bReLU.reshape(1, -1))\n",
    "        self.dL_dw_hidden = np.matmul(self.dL_db_hidden.T, self.embedding_activation.reshape(1, -1))\n",
    "        \n",
    "        hidden_sum = np.sum(np.matmul(self.dL_db_hidden, self.hidden_layer.weights), axis=1, keepdims=True)\n",
    "        delta = np.matmul(hidden_sum.reshape(-1, 1), self.embedding_z.reshape(1, -1))\n",
    "        q = int(delta.shape[1] / self.n_prev_words)\n",
    "        self.dL_db_embedding = np.array([delta[:, i: i+q] for i in range(self.n_prev_words)]).reshape(self.n_prev_words, self.n_embeddings)\n",
    "        self.dL_dw_embedding = np.array([np.matmul(self.dL_db_embedding[i].reshape(-1, 1), self.x[i].reshape(1, -1)) for i in range(self.n_prev_words)])\n",
    "        \n",
    "        # finally, update the weights with the new loss gradients\n",
    "        self._update_weights()\n",
    "    \n",
    "    def _update_weights(self):\n",
    "        # update all the embedding weights\n",
    "        for i in range(self.n_prev_words):\n",
    "            self.embedding_layer.weights -= self.alpha * self.dL_dw_embedding[i]\n",
    "        # update hidden layer\n",
    "        self.hidden_layer.weights -= self.alpha * self.dL_dw_hidden\n",
    "        self.hidden_layer.bias -= self.alpha * self.dL_db_hidden.flatten()\n",
    "        # update output layer\n",
    "        self.output_layer.weights -= self.alpha * self.dL_dw_output\n",
    "        self.output_layer.bias -= self.alpha * self.dL_db_output.flatten()\n",
    "\n",
    "    def fit(self, x, y, n_epochs=1):\n",
    "        self._feedforward(x)\n",
    "        self._backprop(y)\n",
    "            \n",
    "        return self.output_activation\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Use logistic regression\n",
    "        print(\"TODO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ba6e813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116199\n"
     ]
    }
   ],
   "source": [
    "with open(\"metamorphosis.txt\",'r',encoding='utf-8') as file:\n",
    "    raw_metamorphosis = file.read()\n",
    "    \n",
    "replacements = ['\\n', '\\t', '\\r', ';', ':', '.', ',', '  ']\n",
    "\n",
    "stripped_metamorphosis = raw_metamorphosis\n",
    "\n",
    "for rep in replacements:\n",
    "    stripped_metamorphosis = stripped_metamorphosis.replace(rep, ' ')\n",
    "\n",
    "stripped_metamorphosis.lower().split()\n",
    "\n",
    "metamorphosis_vocab = list(set(stripped_metamorphosis))\n",
    "vocab = {}\n",
    "one_hots = {}\n",
    "for i, word in enumerate(metamorphosis_vocab):\n",
    "    one_hot_vector = np.zeros(len(metamorphosis_vocab))\n",
    "    one_hot_vector[i] = 1\n",
    "    vocab[word] = one_hot_vector\n",
    "\n",
    "print(len(stripped_metamorphosis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d78fa1f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116196 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch progression 0/5: 100%|██████████████████████| 19997/19997 [00:01<00:00, 16469.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0088906  0.00565879 0.00783642 0.04437519 0.00495588 0.00572283\n",
      "  0.00519349 0.00887587 0.00685393 0.00931377 0.00819821 0.02095734\n",
      "  0.05796187 0.00682873 0.00857505 0.00747356 0.08132443 0.00720962\n",
      "  0.0073385  0.00679601 0.00563062 0.00538826 0.00499193 0.00504721\n",
      "  0.01069428 0.04890444 0.00640441 0.01099156 0.00666486 0.00748014\n",
      "  0.00959972 0.01100025 0.03934529 0.00852735 0.00792455 0.00683137\n",
      "  0.06142596 0.0046009  0.00926357 0.00750765 0.00820065 0.0393008\n",
      "  0.00785332 0.00552585 0.00575352 0.00826956 0.00840439 0.006749\n",
      "  0.0404274  0.19581078 0.00881982 0.00888992 0.01809751 0.00459549\n",
      "  0.00960965 0.00580648 0.00669488 0.00705164 0.00557489]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch progression 1/5: 100%|██████████████████████| 19997/19997 [00:01<00:00, 16546.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00592361 0.00394747 0.00527226 0.04633366 0.00350556 0.00399601\n",
      "  0.00373478 0.0065386  0.00592472 0.0061738  0.0068567  0.02682902\n",
      "  0.07781399 0.00470383 0.00573399 0.00508431 0.09216104 0.00496321\n",
      "  0.0049852  0.00470273 0.0039314  0.00381627 0.00351615 0.00355114\n",
      "  0.00970802 0.06261868 0.00445148 0.0195325  0.00457625 0.00508297\n",
      "  0.0080796  0.00826663 0.04399504 0.00566502 0.00533587 0.00467971\n",
      "  0.05592976 0.00332479 0.00614183 0.00657821 0.00553133 0.05327202\n",
      "  0.00529525 0.00386391 0.00401921 0.00640538 0.00559298 0.00462746\n",
      "  0.05861889 0.1922314  0.00635399 0.00587352 0.02467019 0.00325938\n",
      "  0.00822338 0.00404334 0.00459912 0.00560592 0.0039475 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch progression 2/5: 100%|██████████████████████| 19997/19997 [00:01<00:00, 16282.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00463694 0.00316241 0.00414699 0.04818591 0.00282689 0.00319941\n",
      "  0.00303801 0.00528024 0.00591282 0.00480107 0.00668329 0.03284603\n",
      "  0.07577496 0.00373894 0.00451756 0.00401926 0.09667494 0.00394843\n",
      "  0.00394008 0.00374418 0.00315128 0.00307653 0.00283083 0.00286043\n",
      "  0.0096455  0.05750098 0.00354457 0.04120202 0.00364216 0.0040252\n",
      "  0.00751832 0.00718367 0.04839209 0.00443206 0.00421362 0.00371479\n",
      "  0.06665147 0.00272761 0.0047788  0.00613914 0.00436461 0.04662774\n",
      "  0.00416359 0.00309856 0.00321707 0.00551663 0.00438278 0.00367455\n",
      "  0.05776255 0.1947155  0.0053409  0.00458602 0.02870231 0.00263483\n",
      "  0.00778846 0.00323761 0.00367269 0.00502215 0.00318203]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch progression 3/5:  82%|██████████████████    | 16447/19997 [00:01<00:00, 16457.32it/s]/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_38424/934040620.py:12: RuntimeWarning: overflow encountered in matmul\n",
      "  x_w = np.matmul(self.weights, x.T)\n",
      "/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_38424/3338405453.py:10: RuntimeWarning: invalid value encountered in subtract\n",
      "  e_x = np.exp(x - max)\n",
      "batch progression 3/5: 100%|██████████████████████| 19997/19997 [00:01<00:00, 16369.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch progression 4/5: 100%|██████████████████████| 19997/19997 [00:01<00:00, 16367.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "v_len = len(metamorphosis_vocab)\n",
    "memory_depth = 3\n",
    "data_set_length = len(stripped_metamorphosis) - memory_depth\n",
    "batch_size = 20000\n",
    "model = Neural_Network(v_len, 32)\n",
    "print(data_set_length, data_set_length // batch_size)\n",
    "for batch in range(data_set_length // batch_size):\n",
    "    dataX = np.zeros((batch_size - memory_depth, memory_depth, v_len), dtype='float16')\n",
    "    dataY = np.zeros((batch_size - memory_depth, v_len), dtype='int16')\n",
    "    for i in range(batch_size - memory_depth):\n",
    "        x = np.zeros((memory_depth, len(vocab)))\n",
    "        for j in range(memory_depth):\n",
    "            x[j] = vocab[stripped_metamorphosis[(batch*batch_size):(batch*batch_size)+batch_size][i+j]]\n",
    "        dataX[i] = x\n",
    "        dataY[i] = vocab[stripped_metamorphosis[(batch*batch_size):(batch*batch_size)+batch_size][i+memory_depth]]\n",
    "        \n",
    "    batch_progress = tqdm(total=dataX.shape[0], desc=\"batch progression {}/{}\".format(batch, data_set_length // batch_size))\n",
    "    for x, y in zip(dataX, dataY):\n",
    "        result = model.fit(x, y)\n",
    "        batch_progress.update()\n",
    "    print(result)\n",
    "    batch_progress.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2a961723",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3395360123.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_37932/3395360123.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    progress = tqdm(total=)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(vocab)\n",
    "dat_len = len(stripped_metamorphosis) - 3 #n_prev_words\n",
    "n_batch = 20000\n",
    "nn_model = Neural_Network(n_vocab, 32)\n",
    "\n",
    "for batch in range(dat_len // n_batch):\n",
    "    x = np.zeroes((n_batch - 3, 3))\n",
    "    \n",
    "    progress = tqdm(total=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "770c804b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward x-shape:  (4, 3, 6)\n",
      "(2, 6) (3, 6)\n",
      "(2, 6) (3, 6)\n",
      "(2, 6) (3, 6)\n",
      "(2, 6) (6, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_37932/279932127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_37932/2555166704.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, n_epochs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_37932/2555166704.py\u001b[0m in \u001b[0;36m_feedforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Multiply by W and pass through ReLU activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_activation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_ReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_37932/3623738115.py\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx_w_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_w\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_w_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 6)"
     ]
    }
   ],
   "source": [
    "data = \"\"\"I've seen what I become, and I cannot let that happen. And for this, you join him? Your destiny can\n",
    "        change just as quickly as the love in one's heart can fade. Nothing is set in stone. But I will cause\n",
    "        so much pain. If there is to be balance, what you have seen must be forgotten. If you're not with me,\n",
    "        then you're my enemy. Only a Sith deals in absolutes. I will do what I must. You will try. It's over,\n",
    "        Anakin! I have the high ground. You underestimate my power. Don't try it!\"\"\"\n",
    "\n",
    "x = np.array([[[0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]]])\n",
    "\n",
    "y = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "model = Neural_Network(6, 2)\n",
    "result = model.fit(x, y)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
