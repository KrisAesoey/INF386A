{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc0bc9",
   "metadata": {},
   "source": [
    "## 1.2 Language Models\n",
    "* Adapt your implementation of logistic regression from Assignment 2 to the implementation of a feedforward neural network (or create a new implementation from scratch). If you use an online implementation as a reference state that in your report and the modifications you have made.\n",
    "* Your neural network should predict upcoming words from prior word context (see Section 7.5 of the reference book)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194523d",
   "metadata": {},
   "source": [
    "## Imports and installation\n",
    "\n",
    "To use this notebook properly you need to Python, as well as numpy and tqdm.\n",
    "To install numpy you can simply \"pip install numpy\", the same goes for tqdm with \"pip install tqdm\"\n",
    "\n",
    "tqdm has not been used to perform any calculations, just enclosed the for loops that are training the models to easily track progress. If you do not wish to use tqdm but still want to run the code, then simply remove the function from the batch_training() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "899a55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from random import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "35bfae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_ReLU(x):\n",
    "    # found on stack overflow\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def backward_ReLU(x):\n",
    "    # found on stack overflow\n",
    "    return (x > 0)\n",
    "\n",
    "def forward_softmax(x):\n",
    "    max = np.max(x, axis=0, keepdims=True)\n",
    "    exp = np.exp(x - max)\n",
    "    sum = np.sum(e_x, axis=0, keepdims=True)\n",
    "    return exp / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1558f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Layer:\n",
    "    \"\"\"Fully Connected Layer without bias.\n",
    "       input x weights -> output\"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_output, seed=1):\n",
    "        ran = np.random\n",
    "        ran.seed(seed)\n",
    "        self.weights = ran.random_sample((n_output, n_input))\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        return np.matmul(self.weights, x.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "51bb4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fully_Connected_Layer:\n",
    "    \"\"\"Fully Connected Layer with bias.\n",
    "       input x weights + bias -> output\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_output, seed=1):\n",
    "        ran = np.random\n",
    "        ran.seed(seed)\n",
    "        self.weights= ran.random_sample((n_output, n_input))\n",
    "        self.bias = ran.random_sample(n_output)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        x_w = np.matmul(self.weights, x.T)\n",
    "        x_w_b = x_w + self.bias\n",
    "        return x_w_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "848ef4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    \"\"\"Neural Network with 3 layers, embedding, hidden and output\"\"\"\n",
    "\n",
    "    # initialize all necessary informatio\n",
    "    def __init__(self, n_vocab, n_embeddings, learning_rate=0.01, n_prev_words=3):\n",
    "        \n",
    "        # information about the model\n",
    "        self.embedding_layer = Embedding_Layer(n_vocab, n_embeddings)\n",
    "        self.hidden_layer = Fully_Connected_Layer(n_embeddings * n_prev_words, n_embeddings)\n",
    "        self.output_layer = Fully_Connected_Layer(n_embeddings, n_vocab)\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "        # z-values representing the results of each layer\n",
    "        # before activating them, generally means when\n",
    "        # the input has benn mutiplied by its weights\n",
    "        self.embedding_z = None\n",
    "        self.hidden_z =  None\n",
    "        self.output_z = None\n",
    "        \n",
    "        # activation of each layer\n",
    "        self.embedding_activation = None\n",
    "        self.hidden_activation = None\n",
    "        self.output_activation = None\n",
    "        \n",
    "        # information about the data\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.n_prev_words = n_prev_words\n",
    "\n",
    "    # updates layers forward\n",
    "    def _feedforward(self, x):\n",
    "        \"\"\"Method from 7.5.1 in Speech and Language Processing.\n",
    "           Step by step commented for easy comparison.\"\"\"\n",
    "        \n",
    "        # create embedding matrix\n",
    "        e = np.array([self.embedding_layer.feedforward(x[i]) for i in range(self.n_prev_words)])\n",
    "        \n",
    "        # create embedding layer e\n",
    "        self.embedding_z = np.concatenate(e, axis=0)\n",
    "        self.embedding_activation = self.embedding_z\n",
    "        # Multiply by W and pass through ReLU activation function\n",
    "        self.hidden_z = self.hidden_layer.feedforward(self.embedding_activation)\n",
    "        self.hidden_activation = forward_ReLU(self.hidden_z)\n",
    "        \n",
    "        # Multiply by U and apply softmax reshaping it into |Vocabulary| x 1\n",
    "        self.output_z = self.output_layer.feedforward(self.hidden_activation)\n",
    "        self.output_activation = forward_softmax(self.output_z).reshape(1, self.n_vocab)\n",
    "\n",
    "    def _backprop(self, x, y):\n",
    "        \n",
    "        # Find the bias of the output layer\n",
    "        # derivative of cross entropy: y_pred - y\n",
    "        self.dL_db_output = (self.output_activation - y) * self.output_activation\n",
    "        self.dL_dw_output = np.matmul(self.dL_db_output.T, self.hidden_activation.reshape(1, -1))\n",
    "\n",
    "        output_sum = np.sum(np.matmul(self.dL_db_output, self.output_layer.weights), axis=1, keepdims=True)\n",
    "        self.hidden_bReLU = backward_ReLU(self.hidden_z)\n",
    "        self.dL_db_hidden = np.matmul(output_sum.reshape(-1, 1), self.hidden_bReLU.reshape(1, -1))\n",
    "        self.dL_dw_hidden = np.matmul(self.dL_db_hidden.T, self.embedding_activation.reshape(1, -1))\n",
    "        \n",
    "        hidden_sum = np.sum(np.matmul(self.dL_db_hidden, self.hidden_layer.weights), axis=1, keepdims=True)\n",
    "        delta = np.matmul(hidden_sum.reshape(-1, 1), self.embedding_z.reshape(1, -1))\n",
    "        q = int(delta.shape[1] / self.n_prev_words)\n",
    "        self.dL_db_embedding = np.array([delta[:, i: i+q] for i in range(self.n_prev_words)]).reshape(self.n_prev_words, self.n_embeddings)\n",
    "        self.dL_dw_embedding = np.array([np.matmul(self.dL_db_embedding[i].reshape(-1, 1), x[i].reshape(1, -1)) for i in range(self.n_prev_words)])\n",
    "        \n",
    "        # finally, update the weights with the new loss gradients\n",
    "        self._update_weights()\n",
    "    \n",
    "    def _update_weights(self):\n",
    "        # clip embeddings to avoid overflow due to normalizing not working\n",
    "        \n",
    "        # update all the embedding weights per previous word\n",
    "        for i in range(self.n_prev_words):\n",
    "            self.embedding_layer.weights -= self.alpha * np.clip(self.dL_dw_embedding[i], -1, 1)\n",
    "            \n",
    "        # update hidden layer weights and bias\n",
    "        self.hidden_layer.weights -= self.alpha * np.clip(self.dL_dw_hidden, -1, 1)\n",
    "        self.hidden_layer.bias -= self.alpha * np.clip(self.dL_db_hidden.flatten(), -1, 1)\n",
    "        \n",
    "        # update output layer weights and bias\n",
    "        self.output_layer.weights -= self.alpha * np.clip(self.dL_dw_output, -1, 1)\n",
    "        self.output_layer.bias -= self.alpha * np.clip(self.dL_db_output.flatten(), -1, 1)\n",
    "\n",
    "    def fit(self, x, y, n_epochs=1):\n",
    "        self._feedforward(x)\n",
    "        self._backprop(x, y)\n",
    "            \n",
    "        return self.output_activation\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self._feedforward(x)\n",
    "        return self.output_layer_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ba6e813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21935\n",
      "3787\n"
     ]
    }
   ],
   "source": [
    "with open(\"metamorphosis.txt\",'r',encoding='utf-8') as file:\n",
    "    raw_metamorphosis = file.read()\n",
    "    \n",
    "replacements = ['\\n', '\\t', '\\r', ';', ':', '.', ',', '\"', \"!\", '?', '-', \"'\", '  ']\n",
    "\n",
    "stripped_metamorphosis = raw_metamorphosis\n",
    "\n",
    "for rep in replacements:\n",
    "    stripped_metamorphosis = stripped_metamorphosis.replace(rep, ' ')\n",
    "\n",
    "stripped_metamorphosis = stripped_metamorphosis.lower().split()\n",
    "\n",
    "metamorphosis_vocab = list(set(stripped_metamorphosis))\n",
    "meta_vocab_dic = {}\n",
    "one_hots = {}\n",
    "for i, word in enumerate(metamorphosis_vocab):\n",
    "    one_hot_vector = np.zeros(len(metamorphosis_vocab))\n",
    "    one_hot_vector[i] = 1\n",
    "    meta_vocab_dic[word] = one_hot_vector\n",
    "\n",
    "print(len(stripped_metamorphosis))\n",
    "print(len(meta_vocab_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f6398c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75258\n",
      "7731\n"
     ]
    }
   ],
   "source": [
    "with open(\"frankenstein.txt\",'r',encoding='utf-8') as file:\n",
    "    raw_frankenstein = file.read()\n",
    "    \n",
    "replacements = ['\\n', '\\t', '\\r', ';', ':', '.', ',', '\"', '  ']\n",
    "\n",
    "stripped_frankenstein = raw_frankenstein\n",
    "\n",
    "for rep in replacements:\n",
    "    stripped_frankenstein = stripped_frankenstein.replace(rep, ' ')\n",
    "\n",
    "stripped_frankenstein = stripped_frankenstein.lower().split()\n",
    "\n",
    "frankenstein_vocab = list(set(stripped_frankenstein))\n",
    "frank_vocab_dic = {}\n",
    "one_hots = {}\n",
    "for i, word in enumerate(frankenstein_vocab):\n",
    "    one_hot_vector = np.zeros(len(frankenstein_vocab))\n",
    "    one_hot_vector[i] = 1\n",
    "    frank_vocab_dic[word] = one_hot_vector\n",
    "\n",
    "print(len(stripped_frankenstein))\n",
    "print(len(frank_vocab_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6c71e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training(model, dataset, vocab, batch_size=5000, n_prev_words=3):\n",
    "    \"\"\"Method that trains a model, its dataet and vocabulary in batches to\n",
    "    avoid memory overflow and give feedback during the training\"\"\"\n",
    "    n_vocab = len(vocab)\n",
    "    n_dataset = len(dataset) - n_prev_words\n",
    "    n_batches = n_dataset // batch_size\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        batch_len = batch_size - n_prev_words\n",
    "        data = np.zeros((batch_len, n_prev_words, n_vocab), dtype='float16')\n",
    "        labels = np.zeros((batch_len, n_vocab), dtype='int16')\n",
    "        for i in range(batch_size - n_prev_words):\n",
    "            x = np.zeros((n_prev_words, n_vocab))\n",
    "            for j in range(n_prev_words):\n",
    "                x[j] = vocab[dataset[(batch*batch_size):(batch*batch_size)+batch_size][i+j]]\n",
    "            data[i] = x\n",
    "            labels[i] = vocab[dataset[(batch*batch_size):(batch*batch_size)+batch_size][i+n_prev_words]]\n",
    "\n",
    "        for x, y in tqdm(zip(data, labels)):\n",
    "            result = model.fit(x, y)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77169c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [00:41, 121.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.71671400e-04 5.27986198e-04 2.19107774e-04 ... 7.90705511e-05\n",
      "  2.25356894e-04 1.84983837e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [00:40, 122.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00016004 0.00037332 0.00025176 ... 0.00026948 0.00016239 0.00016003]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [00:40, 122.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00016004 0.00037332 0.00025176 ... 0.00026948 0.00016239 0.00016003]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [00:40, 123.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00018657 0.00018021 0.00023026 ... 0.00039038 0.00016607 0.00015376]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:22, 60.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.33815302e-04 1.51621135e-04 1.28574313e-04 ... 1.19572633e-04\n",
      "  7.29615087e-05 1.39132604e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:20, 62.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46393285e-04 1.77424015e-04 1.12181759e-04 ... 8.16094752e-05\n",
      "  8.91887916e-05 1.27934529e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:22, 60.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.33965349e-04 2.25824097e-04 7.14055886e-05 ... 1.65125650e-04\n",
      "  4.82831056e-05 2.09951696e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:22, 60.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.13306035e-04 1.69470343e-04 1.37428630e-04 ... 9.39223239e-05\n",
      "  8.39238739e-05 1.26451152e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:25, 58.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.45335323e-04 1.81998995e-04 1.14288060e-04 ... 1.34716901e-04\n",
      "  9.17479814e-05 1.51615043e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:22, 60.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.74418380e-04 2.91943328e-04 1.95299861e-04 ... 1.02458548e-04\n",
      "  9.44232257e-05 1.53354545e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:26, 57.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.12610816e-05 1.94709924e-04 1.07509014e-04 ... 1.16132528e-04\n",
      "  1.19979862e-04 9.79643081e-05]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:22, 60.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.34284751e-04 1.86593882e-04 1.22449251e-04 ... 8.94645863e-05\n",
      "  9.21814890e-05 1.33461874e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [02:21, 35.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.24531923e-04 2.08649219e-04 1.20693881e-04 ... 1.03644450e-04\n",
      "  7.38578216e-05 1.26833850e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [02:29, 33.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.35284453e-04 1.74703729e-04 1.24480507e-04 ... 9.51422783e-05\n",
      "  8.90350539e-05 1.40367589e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [02:30, 33.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.35861156e-04 1.83353042e-04 1.21952736e-04 ... 9.31779847e-05\n",
      "  9.46728620e-05 1.37208885e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:37, 51.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.35284270e-04 1.74703818e-04 1.24480369e-04 ... 9.51428480e-05\n",
      "  8.90350206e-05 1.40367393e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:22, 60.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.33216161e-04 1.52835155e-04 1.01937941e-04 ... 8.10761386e-05\n",
      "  1.03796997e-04 1.21894438e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:18, 63.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18605101e-04 1.70497374e-04 1.10207821e-04 ... 1.14392056e-04\n",
      "  9.16403199e-05 1.65899668e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [01:20, 62.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.35284167e-04 1.74703925e-04 1.24480159e-04 ... 9.51430482e-05\n",
      "  8.90350481e-05 1.40367649e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metamorphosis_model = Neural_Network(len(meta_vocab), 64)\n",
    "batch_training(metamorphosis_model, stripped_metamorphosis, meta_vocab_dic)\n",
    "\n",
    "frankenstein_model = Neural_Network(len(frank_vocab), 64)\n",
    "batch_training(frankenstein_model, stripped_frankenstein, frank_vocab_dic, batch_size=7000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe19f87",
   "metadata": {},
   "source": [
    "## Store the embeddings in TSV files\n",
    "This is done so we get files that we can use in the Embedding Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2a961723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3787)\n",
      "3787 3787\n"
     ]
    }
   ],
   "source": [
    "out_metamorphosis_v = open('vectors_m.tsv', 'w', encoding='utf-8')\n",
    "out_metamorphosis_m = open('metadata_m.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(metamorphosis_vocab):\n",
    "    # get the index of all the dimensions to form a single vector\n",
    "    out_metamorphosis_v.write('\\t'.join([str(x) for x in metamorphosis_model.embedding_layer.weights[:, index]]) + \"\\n\")\n",
    "    out_metamorphosis_m.write(word + \"\\n\")\n",
    "out_metamorphosis_v.close()\n",
    "out_metamorphosis_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d1026c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_frankenstein_v = open('vectors_f.tsv', 'w', encoding='utf-8')\n",
    "out_frankenstein_m = open('metadata_f.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(frankenstein_vocab):\n",
    "    out_frankenstein_v.write('\\t'.join([str(x) for x in frankenstein_model.embedding_layer.weights[:, index]]) + \"\\n\")\n",
    "    out_frankenstein_m.write(word + \"\\n\")\n",
    "out_frankenstein_v.close()\n",
    "out_frankenstein_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e37e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
