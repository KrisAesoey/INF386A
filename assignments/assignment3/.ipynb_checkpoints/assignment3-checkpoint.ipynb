{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc0bc9",
   "metadata": {},
   "source": [
    "## 1.2 Language Models\n",
    "* Adapt your implementation of logistic regression from Assignment 2 to the implementation of a feedforward neural network (or create a new implementation from scratch). If you use an online implementation as a reference state that in your report and the modifications you have made.\n",
    "* Your neural network should predict upcoming words from prior word context (see Section 7.5 of the reference book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "899a55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from random import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "35bfae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(self, y, y_pred):\n",
    "    return -y * math.log(y_pred) + (1 - y) * math.log(y_pred)\n",
    "\n",
    "def forward_ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def backward_ReLU(x):\n",
    "    return max(np.ones(x.shape), 0)\n",
    "\n",
    "def forward_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivitive of sigmoid\n",
    "def backward_sigmoid(x):\n",
    "    return forward_sigmoid(x) * (1 - forward_sigmoid(x))\n",
    "\n",
    "def forward_softmax(z):\n",
    "    return [(exp(zi)/sum([exp(zj) for zj in z])) for zi in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1558f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Layer:\n",
    "    \"\"\"Fully Connected Layer without bias.\n",
    "       input x weights -> output\"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_output, seed=1):\n",
    "        ran = np.random\n",
    "        ran.seed(seed)\n",
    "        self.weights = ran.random_sample((n_output, n_input))\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        return np.matmul(self.weights, x.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "51bb4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Layer:\n",
    "    \"\"\"Connected layer with bias.\n",
    "       input x weights + bias -> output\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_output, seed=1):\n",
    "        ran = np.random\n",
    "        ran.seed(seed)\n",
    "        self.weights= ran.random_sample((n_output, n_input))\n",
    "        self.bias = ran.random_sample(n_output)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        print(x.shape, self.weights.shape)\n",
    "        x_w = np.matmul(self.weights, x.T)\n",
    "        x_w_b = x_w + self.bias\n",
    "        return x_w_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "848ef4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer Neural Network\n",
    "class Neural_Network:\n",
    "\n",
    "    # initialize all necessary informatio\n",
    "    def __init__(self, n_vocab, n_embeddings, learning_rate=0.1, n_prev_words=3):\n",
    "        \n",
    "        # information about the model\n",
    "        self.embedding_layer = Embedding_Layer(n_vocab, n_embeddings)\n",
    "        self.hidden_layer = Linear_Layer(n_embeddings * n_prev_words, n_embeddings)\n",
    "        self.output_layer = Linear_Layer(n_embeddings, n_vocab)\n",
    "        self.gradients= {}\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "        # z-values representing the results of each layer\n",
    "        # before activating them, generally means when\n",
    "        # the input has benn mutiplied by its weights\n",
    "        self.embedding_z = None\n",
    "        self.hidden_z =  None\n",
    "        self.output_z = None\n",
    "        \n",
    "        self.embedding_activation = None\n",
    "        self.hidden_activation = None\n",
    "        self.output_activation = None\n",
    "        \n",
    "        # information about the data\n",
    "        # is updated when \n",
    "        self.vocab = None\n",
    "        self.one_hot_vectors = None\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_prev_words = n_prev_words\n",
    "\n",
    "    # Maybe use for prediction and encoding data\n",
    "    def _create_vocab(self, x):\n",
    "        new_vocab = []\n",
    "        for word in x:\n",
    "            if word not in new_vocab:\n",
    "                new_vocab.append(x)\n",
    "        self.vocab = new_vocab\n",
    "        self.one_hot_vectors = [np.zeros(len(self.vocab)) for _ in range(len(self.vocab))]\n",
    "        for i in range(len(self.vocab)):\n",
    "            self.one_hot_vectors[i][i] = 1  \n",
    "\n",
    "    # updates layers forward\n",
    "    def feedforward(self, x):\n",
    "        \"\"\"Method from 7.5.1 in Speech and Language Processing.\n",
    "           Step by step commented for easy comparison.\"\"\"\n",
    "        e = np.array([self.embedding_layer.feedforward(x[i, :]) for i in range(self.n_prev_words)])\n",
    "        print(\"E shape: \", e.shape)\n",
    "        # create embedding layer e\n",
    "        self.embedding_z = np.concatenate(e, axis=0)\n",
    "        print(\"E concat shape: \", self.embedding_z)\n",
    "        self.embedding_activation = self.embedding_z\n",
    "        # Multiply by W and pass through ReLU activation function\n",
    "        self.hidden_z = self.hidden_layer.feedforward(self.embedding_activation)\n",
    "        self.hidden_activation = forward_ReLU(self.hidden_z)\n",
    "        \n",
    "        # Multiply by U and apply softmax reshaping it into |Vocabulary| x 1\n",
    "        self.output_z = self.output_layer.feedforward(self.hidden_activation)\n",
    "        self.output_activation = forward_softmax(self.output_z).reshape(1, self.n_vocab)\n",
    "\n",
    "    def backprop(self, y):\n",
    "        # step by step what to dodo\n",
    "        \n",
    "        #Find the cross entropy loss of the output layer\n",
    "        output_delta = cross_entropy(y, self.output_activation)\n",
    "        \n",
    "        # Reshape this to fit the hidden layer results\n",
    "        self.output_loss_weights = np.matmul(output_delta, self.hidden_activation.reshape(1, -1))\n",
    "        \n",
    "        \n",
    "        # finally, update the weights with the new loss gradients\n",
    "        self._update_weights()\n",
    "        \n",
    "        \n",
    "    def _gradient(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        loss = y_pred - y\n",
    "        gradients = [x[i] * loss for i in range(len(x))]\n",
    "        return gradients, loss\n",
    "    \n",
    "    def _update_weights(self, x, y):\n",
    "        for i in range(self.n_prev_words):\n",
    "            self.embedding_layer.weights -= self.alpha * self.embedding_loss_weights[i]\n",
    "        self.hidden_layer.weights -= self.alpha * self.hidden_loss_weights\n",
    "        self.hidden_layer.bias -= self.alpha * self.hidden_loss_bias\n",
    "        self.output_layer.weights -= self.alpha * self.output_loss_weights\n",
    "        self.output_layer.bias -= self.alpha * self.output_loss_bias\n",
    "\n",
    "    def fit(self, x, n_epochs=1):\n",
    "        #self._create_vocab(x)\n",
    "        \n",
    "        y_pred = self.feedforward(x)\n",
    "            \n",
    "        return self.output_activation\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Use logistic regression\n",
    "        print(\"TODO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "770c804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E shape:  (3, 2, 3)\n",
      "E concat shape:  [[7.20324493e-01 1.14374817e-04 9.23385948e-02]\n",
      " [3.45560727e-01 3.96767474e-01 6.85219500e-01]\n",
      " [3.02332573e-01 9.23385948e-02 9.23385948e-02]\n",
      " [5.38816734e-01 6.85219500e-01 6.85219500e-01]\n",
      " [4.17022005e-01 1.46755891e-01 9.23385948e-02]\n",
      " [1.86260211e-01 4.19194514e-01 6.85219500e-01]]\n",
      "(6, 3) (2, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_25996/2471101905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_25996/1004189075.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, n_epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m#self._create_vocab(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_25996/1004189075.py\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Multiply by W and pass through ReLU activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_activation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_ReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/km/967q2mlj1c79my7z2clmws2c0000gn/T/ipykernel_25996/978128266.py\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx_w_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_w\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_w_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 6)"
     ]
    }
   ],
   "source": [
    "data = \"\"\"I've seen what I become, and I cannot let that happen. And for this, you join him? Your destiny can\n",
    "        change just as quickly as the love in one's heart can fade. Nothing is set in stone. But I will cause\n",
    "        so much pain. If there is to be balance, what you have seen must be forgotten. If you're not with me,\n",
    "        then you're my enemy. Only a Sith deals in absolutes. I will do what I must. You will try. It's over,\n",
    "        Anakin! I have the high ground. You underestimate my power. Don't try it!\"\"\"\n",
    "\n",
    "x = np.array([[[0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]],\n",
    "                      [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]]])\n",
    "\n",
    "model = Neural_Network(6, 2)\n",
    "result = model.fit(x)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6e813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
